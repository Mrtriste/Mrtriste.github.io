---
layout:     post
title:      "提升树"
subtitle:   "分类问题和回归问题的提升树"
date:       2017-05-01
author:     "MrTriste"
header-img: "img/post-bg-js-module.jpg"
tags:
    - 提升树
    - 分类
    - 回归
    - 人工智能
    - 机器学习
---


## Boosting Tree

提升树，是一种以决策树为基函数的提升方法。

它的基本思想是每次都对前面一棵树学习的残差再次进行学习，而不是与前面的学习过程是独立的，每一棵树学的是之前所有树结论和的残差**，这个**残差就是一个加预测值后能得真实值的累加量。比如A的真实年龄是18岁，但第一棵树的预测年龄是12岁，差了6岁，即残差为6岁。那么在第二棵树里我们把A的年龄设为6岁去学习，如果第二棵树真的能把A分到6岁的叶子节点，那累加两棵树的结论就是A的真实年龄；如果第二棵树的结论是5岁，则A仍然存在1岁的残差，第三棵树里A的年龄就变成1岁，继续学。

分类问题的过程就是AdaBoost提升方法的过程，回归问题的过程是前向分布加法算法的特例，有以下三种：

1. 回归问题：平方误差损失函数，决策树的类型就是回归树。
2. 分类问题：指数损失函数，决策树的类型就是分类树。
3. 一般决策问题：一般损失函数



## 分类问题的提升树

(1). 初始化权值分布，

$$D_1 = {w_{11},w_{12},...,w_{1n}}$$

(2).迭代过程

a.根据权值分布$$D_m$$找到分类误差率最小的切分点，得到基本分类器（这里对应的应该就是回归问题的计算损失函数的部分，分类问题没有用到具体的损失函数，就是根据分类误差率的大小判断）

$$G_m(x):X->{-1,+1}$$

比如:

$$\begin{equation}G_1(X)=\begin{cases}1&x<2.5\\ -1&x>2.5\end{cases}\end{equation}$$

b.计算$$G_m$$分类器的误差率：

$$e_m=\sum_{i=1}^Nw_{mi}I(G_m(x_i)\ne y_i)$$

c.计算$$G_m(x)$$的系数:

$$\alpha_m=\frac12log\frac{1-e_m}{e_m}$$

d.更新权值分布$$D_{m+1}$$，用于下次迭代的a步，具体的更新公式就不展开了。

(3).最终的分类器：

$$G(x) = sign(f(x)) = sign(\sum_{m=1}^{M}\alpha _mG_m(x))$$

*PS*：在(2)的a,b部分，我们使用的是分类误差率作为选择最优特征的标准，其实还有Gini指数、熵这些也可以作为标准，在二分类问题中，基尼指数、熵之半与分类误差率很接近。



## 回归问题的提升树：

我们首先要知道我们接下来的目的是什么，我们知道如果将输入空间划分为$$J$$个互不相交的区域$$R_1,R_2,..,R_J$$，并且在每个区域上确定输出的常量$$c_j$$，那么树可以表示为

$$T(x;o_m)=\sum_{j=1}^Jc_jI(x\in R_j)$$

我们的```目的```就是找到这样的```划分点```和$$c_j$$，将其划分成若干个不想交的区域。

以下是具体的过程：

(1).初始化$$f_0(x) = 0$$

(2).迭代过程，对于$$m=1,2,...,M$$

a.计算残差(在m=1时，初始样本数据集就可以当成残差，因此不需要计算)

$$r_{mi}=y_i-f_{m-1}(x_i),i=1,2,3,...,N $$

b.拟合残差$$r_{mi}$$学习一个回归树，得到$$T(x;o_m)$$

对于二分类回归问题，这里的具体过程是：

1. 遍历切分点，根据以下的公式求解最优的切分点(note:注意这里符合我们上述说的目的)

2. 根据上面求得的切分点，将N个残差数据分裂成两个部分$$R1,R2$$，我们可以根据定义好的损失函数计算出代表$$R_1$$的输出常量$$c_1$$和代表$$R_2$$的输出常量$$c_2$$来使损失达到最小，一般常用的损失函数为平方损失函数，容易计算得($$N_1,N_2分别是R_1，R_2的个数$$)：

   $$c_1 = \frac{1}{N_1}\sum_{x_i\in R_1}y_i,c_2 = \frac{1}{N_2}\sum_{x_i\in R_2}y_i$$

3. 得到拟合残差的回归树，形如：

$$\begin{equation}G_1(X)=\begin{cases}-0.52&x<6.5\\ 0.22&x>6.5\end{cases}\end{equation}$$

c.更新$$f_m(x) = f_{m-1}+T(x;o_m)$$

(3).最终的分类器：

$$f_M(x) = \sum_{m=1}^MT(x;o_m)$$

