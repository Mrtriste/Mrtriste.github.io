---
layout:     post
title:      "决策树"
subtitle:   "特征选择 ID3 C4.5 CART"
date:       2017-03-16
author:     "MrTriste"
header-img: "img/post-bg-js-module.jpg"
tags:
    - 决策树
    - 人工智能
    - 机器学习
---

## 决策树

决策树有两种，分类决策树和回归决策树。

我们这里主要讲的是分类决策树。

分类决策树就是将训练集按照某个特征在当前节点生成若干个分叉，每个分叉代表不同的属性，把属于对应属性的节点分别放在新生成的对应的节点中，直到节点不能再分（只剩一个节点或者节点属于同一类），然后根据当前节点含有的数据所在类进行多数表决等方式确定该节点的类，此节点也作为叶节点。由于可能过拟化，最后要进行剪枝。

因此决策树算法一般包括特征选择、决策树生成、剪枝。



## 特征选择

训练集中的数据有这么多维度的特征，应该选择什么样的特征来进行分类呢？

直观上，我们感觉可以选择最明显的特征分类，也就是说，根据这个特征分类后，下次输入测试实例时，根据这个特征判断它的类型，我可以很确定。

#### 熵

为了描述这个属性，我们首先定义熵，来度量随机变量的不确定性。

一个有限离散随机变量的熵表示为

$$
H(X)=H(p)=-\sum_{i=1}^{n}{p_ilogp_i}
$$

p为随机变量X的概率分布，熵越大，随机变量X的不确定性就越大。

当熵中的概率由数据估计得到时，也叫经验熵。

#### 条件熵

条件熵$$H(X\mid Y)$$表示在随机变量X给定的条件下，随机变量Y的条件熵，也就是X给定时，Y的条件概率分布的熵对X的数学期望，公式为

$$
H(Y\mid X)=\sum_{i=1}^{n}p_iH(Y\mid X=x_i)\quad ,其中p_i=P(X=x_i)
$$

公式可以理解为将Y按照$$p_i$$分成n份，分别求了熵再乘以比例，最后加起来。

条件熵可以表示在已知随机变量X的条件下随机变量Y的不确定性。

当条件熵中的概率由数据估计得到时，也叫条件经验熵。

#### 信息增益

已知集合D的经验熵H(D)，给定特征A下D的经验条件熵为H(D|A)，将它们作差，也就是

$$
g(D,A)=H(D)-H(D|A)=-\sum_{k=1}^K\frac{\mid C_k\mid}{\mid D\mid}log_2\frac{\mid C_k\mid}{\mid D\mid}-\sum_{i=1}^n\frac{\mid D_i\mid}{\mid D\mid}·(-\sum_{k=1}^K\frac{\mid D_{ik}\mid}{\mid D_{i}\mid}log_2\frac{\mid D_{ik}\mid}{\mid D_{i}\mid})
$$

$$
将D根据特征A分成n个D_i,D_{ik}表示D_i集合中属于C_k类的集合,\mid\quad\mid 表示集合个数
$$

我们将它称作信息增益，代表集合D因为特征A而减少的不确定性。

我们选取特征时，应该对每个特征计算信息增益，选取信息增益最大的特征。

#### 信息增益比

一个特征的取值集合越多，那么它的信息增益更趋向于大，这可能对我们的判断造成影响，因此可以使用信息信息增益比作为另一特征选择准则。

已知信息增益$$g(D,A)$$和D关于特征A的熵$$H_A(D)=-\sum_{i=1}^n\frac{\mid D_i\mid}{\mid D\mid}log_2\frac{\mid D_i\mid}{\mid D\mid}$$

信息增益比为

$$
g_R(D,A)=\frac{g(D,A)}{H_A(D)}
$$


## 决策树的生成

### ID3算法

其思想是计算特征集A（注意：刚才的A为单个特征，现在的A是包含n个维度特征的特征集）中所有的特征的信息增益，选择最大的$$A_g$$，如果不小于阈值，则根据$$A_g$$的各个值分成若干非空集$$D_i$$，对每个$$D_i$$构建子节点，对实例数最大的类作为标记，然后对每个子节点除去已选过的特征作为特征集递归上述过程。

### C4.5算法

将上述算法的信息增益改成信息增益比。



## 剪枝

#### 损失函数

为了避免出现过拟合的现象，我们要对决策树进行剪枝。

设决策树的子节点集合为T，t是T中的一个元素，该叶节点有$$N_t$$个样本，其中k类的样本有$$N_{tk}$$个，共K个分类

$$H_t(T)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$$

则损失函数可以定义为

$$
C_{\alpha}(T)=\sum_{t=1}^{\mid T\mid}N_tH_t(T)+\alpha\mid T\mid
$$

右边第一项表示误差大小，第二项表示模型的复杂度，也就是用叶节点表示，防止过拟化。（一般的损失函数都用两项来表示，误差和模型复杂度，具体可以参看另一篇文章[损失函数有感](https://mrtriste.github.io/2017/03/15/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/)）

损失函数就是在精确度和过拟化之间找到一个平衡。第二项很容易理解，$$\alpha$$为参数，控制两者之间的影响，较大的$$\alpha$$促使选择简单的模型。第一项怎么理解呢？

对每个叶节点t来说，$$H_t(T)$$表示t的熵（也就是不确定性）的期望，针对的是t子节点中每个数据实例的熵的期望，t子节点中有$$N_t$$个实例，那么t子节点总的熵（不确定性）就是$$N_tH_t(T)$$，整个树有$$\mid T\mid$$个叶节点，加起来就是整棵树的熵（不确定性，也可以理解成误差）。

#### 剪枝算法

计算每个节点的经验熵（非叶结点的经验熵为以该子节点为根节点的字树包含的所有节点的经验熵），用叶节点回缩到父节点后的损失函数值与回缩之间的比较，如果回缩后的小，则进行剪枝，直到不能继续为止。



## CART算法

CART是一种分类与回归树，它假设决策树是二叉树。

它的思想是每一次分类将当前特征只分成两种情况，要么满则要么不满足，比如要么使用真实头像，要么不使用，或者要么大于100，要么小于100，而不是0-50,50-100,100-200，它只分成两种情况，接下来对剩下的分类继续分叉，比如之后将<100的分成0-50和50-100.

### 1.回归树的生成

其思想是将当前节点的数据按照某个特征在某个切分点分成两类，比如$$R_1,R_2$$，其对应的类别为$$C_1,C_2$$，我们的任务就是找到一个切分点使误差最小，那么怎么度量误差呢？这里使用的是平方误差，即

$$
min[min\sum_{x_i\in R_1}(y_i-c_1)^2+min\sum_{x_i\in R_2}(y_i-c_2)^2]
$$

遍历某个特征可取的s个切分点（对离散型变量，要么等于要么不等于；对连续型变量，<或者>=），选择使上式最小的切分点。

用选定的切分点划分该节点为若干个$$R_m$$，该节点的类别为划分后所有数据类别的平均值，即

$$
\frac{1}{N_m}\sum_{x_i\in R_m}y_i
$$

继续对两个子节点调用上述步骤。



### 2.分类树的生成

其思想类似于回归树，只是它用的是基尼指数来选择最优特征。

#### 基尼指数

概率分布的基尼指数为

$$
Gini(p)=\sum_{i=1}^np_i(1-p_i)=1-\sum_{i=1}^np_k^2
$$

假设有K个类，$$C_1,C_2,...,C_K$$，给定的集合D的基尼指数为

$$
Gini(D)=1-\sum_{k=1}^K(\frac{\mid C_k\mid}{\mid D\mid})^2
$$

对CART算法，D被特征A的某个值分成两类D1，D2，则在特征A的条件下，集合D的基尼指数为

$$
Gini(D,A)=\frac{\mid D_1\mid}{\mid D\mid}Gini(D_1)+\frac{\mid D_2\mid}{\mid D\mid}Gini(D_2)
$$

基尼指数与熵类似，表示一个集合的不确定性，基尼数值越大，不确定性就越大。

我们选取特征的时候就应该选取基尼指数最小的特征。

#### 生成算法

在生成CART决策树时，我们计算按照特征A中所有的值分割的基尼指数，选取基尼指数最小的切分点作为切分点，将该节点分成两个子节点，再对这两个子节点递归调用刚才的算法，直到不能再分为止。



### 3.CART剪枝

损失函数为

$$
C_\alpha (T)=C(T)+\alpha \mid T\mid
$$

生成算法略，我也没太懂，有时间补。





