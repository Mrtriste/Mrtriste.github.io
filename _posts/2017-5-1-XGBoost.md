---
layout:     post
title:      "XGBoost"
subtitle:   "XGBoost"
date:       2017-05-01
author:     "MrTriste"
header-img: "img/post-bg-js-module.jpg"
tags:
    - 机器学习
    - XGBoost
---


参考[xgboost原理](http://blog.csdn.net/a819825294/article/details/51206410)



## 目标函数

XGBoost目标函数的定义就是：损失函数+正则项+常量。

正则项就是为了控制模型的复杂度，损失函数本来是预测的分类$$\hat{y_i}$$与目标$$y_i$$的差异，但是为了降低计算的复杂度，基于boosting的思想，每次根据上一次的残差更新这次预测值，那么损失函数就是$$(\hat{y_i}+f_t(x_i))$$与$$y_i$$的差，然后用泰勒展开来近似这个损失函数，最终损失函数只依赖于每个数据点在误差函数上的一阶导数和二阶导数。



## 简化目标函数

对于$$f_t(x)$$，我们可以将它细分，用$$f_t(x)=w_{q(x)}$$来表示，其中$$q(x)$$表示样本x所在的叶子索引，$$w_i$$表示叶子i的值是多少，最后可以简化成

$$Obj^{(t)} = \sum_{j=1}^T[G_jw_j+\frac{1}{2}(H_j+\lambda)w_i^2]+\gamma T$$

其中$$G_j$$和$$H_j$$分别是误差函数的一阶导和二阶导，$$T$$是叶子节点数。



##  分裂节点算法

###    1、贪心法

这种算法针对数据量不是那么大，因此遍历到了每个节点，也更精确。

假设有m个叶子节点，现在就其中某个叶子节点分裂的情况而言：

$$Gain = \frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}]-\lambda$$

在节点的某个位置分裂时，会形成左子树和右子树，[]中前两项分别是左右子树的分数，减去原来的分数，又由于分裂后多了一个节点，所以要减去$$1*\lambda$$

这样就可以得到由于在该位置分裂，整个树的分数增加了多少，取最大的即可。

###    2、近似算法

在数据量很大时，数据不能一次性的读入内存，那么就需要使用近似算法来计算，具体思想是根据特征分布的百分比提出候选的点，然后统计数据来找到最好的方案。

这个算法有两个变种，一种是全局的，一种是本地的，区别就是什么时候提出候选点。全局方案是在一开始初始化时就提出所有的候选点，本地的是在每次分裂之后提出候选点。与本地方案比，全局方案需要更少的提出步骤。但是，通常来说，全局方案也需要更多的候选点，因为一旦proposed后，这些候选点就不变了，但是本地方案每次分裂后都会重新提出，所以在有更深深度的树中，local proposal会提出更适合的点。





