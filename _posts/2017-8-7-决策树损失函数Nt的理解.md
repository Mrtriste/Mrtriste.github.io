---
layout:     post
title:      "决策树损失函数Nt的理解"
subtitle:   "决策树损失函数Nt的理解"
date:       2017-08-07
author:     "MrTriste"
header-img: "img/post-bg-js-module.jpg"
tags:
    - 决策树
    - 人工智能
    - 机器学习
---

### 决策树损失函数对Nt的理解

为了避免出现过拟合的现象，我们要对决策树进行剪枝。

设决策树的子节点集合为T，t是T中的一个元素，该叶节点有$N_t$个样本，其中k类的样本有$N_{tk}$个，共K个分类
$$
H_t(T)=-\sum_{k=1}^K\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}
$$


则损失函数可以定义为
$$
C_{\alpha}(T)=\sum_{t=1}^{\mid T\mid}N_tH_t(T)+\alpha\mid T\mid
$$
右边第一项表示误差大小，第二项表示模型的复杂度，也就是用叶节点表示，防止过拟化。（一般的损失函数都用两项来表示，误差和模型复杂度，具体可以参看另一篇文章[损失函数有感](https://mrtriste.github.io/2017/03/15/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/)）

损失函数就是在精确度和过拟化之间找到一个平衡。第二项很容易理解，$\alpha$为参数，控制两者之间的影响，较大的$\alpha$促使选择简单的模型。

第一项怎么理解呢？为什么要乘以Nt呢？

- 理解1

首先问一个问题，Ht(T)代表的是什么？你肯定会说是经验熵，那什么是经验熵，你肯定会说是不确定度，到这里都没错，那这个不确定度是什么的不确定度呢？

我的理解是，这个叶子节点内部取k个类的不确定度，注意是节点【内部】的不确定度，每个叶子节点可以看作是独立的，既然是内部的事情，凭什么暴力的将各个内部的不确定度相加，我们至少到同一个级别的平台再加吧。

不知道你现在有没有感觉暴力的相加确实少了点什么，我的理解是，少了该节点的样本数，也就是Nt。不知道你有没有注意到，信息熵只用到了概率，而忽略了样本数，也就是只关注内部各个类别的比例，而不在乎整体数量的多少，那么乘以Nt后，我们把它叫做不确定次数，不确定程度就是不确定次数归一化后的东西。

既然都这么暴力了，就更暴力一点，你把Ht(T)理解成频率，Nt*Ht(T)对应地理解成次数吧。比如有A股B股两支股票，A股买了10次，赚了7次，B股买了100次，赚了50次，赚的频率分别是0.7和0.5，那么计算你投资的能力，是0.7+0.5更有意义呢还是7+50更有意义呢？我觉得7+50更有意义吧。

虽然不确定性和不确定次数并非频率和次数，但它们的相对关系就这么理解吧。

- 理解2

对每个叶节点t来说，$H_t(T)$表示t的熵（也就是不确定性）的期望，针对的是t子节点中每个数据实例的熵的期望，t子节点中有$N_t$个实例，那么t子节点总的熵（不确定性）就是$N_tH_t(T)$，整个树有$\mid T\mid$个叶节点，加起来就是整棵树的熵（不确定性，也可以理解成误差）。