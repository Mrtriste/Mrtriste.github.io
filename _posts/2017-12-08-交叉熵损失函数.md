---
layout:     post
title:      "交叉熵损失函数"
subtitle:   "sigmoid，softmax，推导，好处"
date:       2017-12-08
author:     "MrTriste"
header-img: "img/post-bg-js-module.jpg"
tags:
    - 损失函数
    - 交叉熵
    - softmax
    - sigmoid
    - 人工智能
    - 机器学习
---

## 交叉熵损失函数

以前没有特别注意这个东西，今天看XGBoost的时候，看到它里面classification的softmax的代码，不知道为什么这里可以用softmax，但是看到softmax这个函数非常亲切，看PRML时，里面的LR部分提到过，跟sigmoid函数差不多性质，但是类比过来还是有些问题，本质上还是没有深刻理解。

sigmoid函数用于二分类，softmax用于多分类。

下面做一个总结。

LR里面计算loss function 时，用的是交叉熵损失函数。

我们首先要知道sigmoid和softmax函数是可以作为概率的（高斯分布的前提下，具体说明可见PRML第四章）

1. 对第$$i$$个样本的交叉熵损失函数：
   $$
   loss=-\sum_k y_kln\ p_k\\
   p_k = \sigma(z_k)\\
   z_k=w_kx_i+b_k
   $$
   这里要说一下$$z_k$$，$$z_k$$ 是我们的预测值，比如这里可以是$$w_kx_i+b_k$$ ，在XGBoost中可以直接是各个叶子节点的score和；

   $$y_k$$代表是否是第$$k$$类，只能为0或1；

   $$p_k$$是我们用softmax计算得到的这个样本的特征判断为第$$k$$类的概率；

   $$x_i$$是第$$i$$ 个样本；

   $$w_k,b_k$$ 是第$$k$$ 类的参数(具体可见PRML4.2.1)。


2. softmax:

$$
\sigma(z_i)=\frac{e^{z_i}}{\sum_k{e^{z_k}}}
$$

3. 我们对样本$$x_i$$ 分别预测它关于每个类别的预测值$$z_k$$ ，同时可以计算概率，但我们要根据预测值$$z_k$$ 计算损失函数loss，然后更新相应参数来减小误差，因此要求导。

   下面对loss关于预测值$$z_i$$求导：

   求导可参见https://www.zhihu.com/question/23765351/answer/240869755

   因为一个样本只属于一个类，因此loss只有一个$$y_k$$ 不为0，其他都为0，所以我们只需要计算第i个样本的真实类别的loss。

   - 第一种情况：k == i
     $$
     \begin{align}
     \frac{\partial loss}{\partial z_i}&=\frac{\partial(-ln\ p_k)}{\partial z_i}\\&=
     -\frac{1}{\sigma(z_i)}*\frac{\partial z_i\ \{e^{z_i}*\frac{1}{\sum_j e^{z_j}}\}}{\partial z_i}\\
     &=-\frac{1}{\sigma(z_i)}*\sigma(z_i)[1-\sigma(z_i)]\\
     &=\sigma(z_i)-1
     \end{align}
     $$

   - 第二种情况：k != i
     $$
     \begin{align}
     \frac{\partial loss}{\partial z_i}&=\frac{\partial(-ln\ p_k)}{\partial z_i}\\&=
     -\frac{1}{\sigma(z_k)}*\frac{\partial z_i\ \{e^{z_k}*\frac{1}{\sum_j e^{z_j}}\}}{\partial z_i}\\
     &=-\frac{1}{\sigma(z_k)}*-\sigma(z_k)*\sigma(z_i)\\
     &=\sigma(z_i)
     \end{align}
     $$
     ​


- 交叉熵有什么好处？

  使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。

  一般的均方误差$$\frac{1}{2}(y-\sigma(z_i)^2)$$求导后有$$\sigma(z_i)^{'}$$ ，很多部分都接近于0，用导数来更新参数会很慢，因此用交叉熵误差函数会改变这一点，因为导数是$$\sigma(z_i)(1-\sigma(z_i))$$， 只与误差有关。

  具体可见这篇文章http://blog.csdn.net/u012162613/article/details/44239919



### Reference

[交叉熵代价函数](http://blog.csdn.net/u012162613/article/details/44239919)

[如何通俗的解释交叉熵与相对熵?](https://www.zhihu.com/question/41252833/answer/108777563)

[交叉熵损失函数的推导](http://blog.csdn.net/jasonzzj/article/details/52017438)

