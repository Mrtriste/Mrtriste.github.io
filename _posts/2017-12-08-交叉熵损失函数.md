---
layout:     post
title:      "交叉熵损失函数"
subtitle:   "sigmoid，softmax，推导，好处"
date:       2017-12-08
author:     "MrTriste"
header-img: "img/post-bg-js-module.jpg"
tags:
    - 损失函数
    - 交叉熵
    - softmax
    - sigmoid
    - 人工智能
    - 机器学习
---

## 交叉熵损失函数

以前没有特别注意这个东西，今天看XGBoost的时候，看到它里面classification的softmax的代码，不知道为什么这里可以用softmax，但是看到softmax这个函数非常亲切，看PRML时，里面的LR部分提到过，跟sigmoid函数差不多性质，但是类比过来还是有些问题，本质上还是没有深刻理解。

sigmoid函数用于二分类，softmax用于多分类。

下面做一个总结。

LR里面计算loss function 时，用的是交叉熵损失函数。

我们首先要知道sigmoid和softmax函数是可以作为概率的（高斯分布的前提下，具体说明可见PRML第四章）

1. 对第$$i$$个样本的交叉熵损失函数：

$$
loss=-\sum_k y_kln\ p_k\\
p_k = \sigma(z_k)\\
z_k=w_kx_i+b_k
$$

这里要说一下$$y_k$$，$$y_k$$代表是否是第$$k$$类，只能为0或1，$$p_k$$是我们用softmax计算得到的这个样本的特征判断为第$$k$$类的概率，$$x_i$$是第$$i$$ 个样本，$$w_k,b_k$$ 是第$$k$$ 类的参数(具体可见PRML4.2.1)。

2. softmax:

$$
\sigma(z_i)=\frac{e^{z_i}}{\sum_k{e^{z_k}}}
$$

3. 下面对loss求导：

   因为一个样本只属于一个类，因此loss只有一个$$y_k$$ 不为0，其他都为0，所以我们只需要计算第i个样本的真实类别的loss。
   $$
   \frac{\partial loss}{\partial z_i}=\frac{\partial z_i\ \{e^{z_i}*\frac{1}{\sum_ke^{z_k}}\}}{\partial z_i}=\sigma(z_i)[1-\sigma(z_i)]
   $$


- 交叉熵有什么好处？

  使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。

  一般的均方误差$$\frac{1}{2}(y-\sigma(z_i)^2)$$求导后有$$\sigma(z_i)^{'}$$ ，很多部分都接近于0，用导数来更新参数会很慢，因此用交叉熵误差函数会改变这一点，因为导数是$$\sigma(z_i)(1-\sigma(z_i))$$， 只与误差有关。

  具体可见这篇文章http://blog.csdn.net/u012162613/article/details/44239919



### Reference

[交叉熵代价函数](http://blog.csdn.net/u012162613/article/details/44239919)

[如何通俗的解释交叉熵与相对熵?](https://www.zhihu.com/question/41252833/answer/108777563)

[交叉熵损失函数的推导](http://blog.csdn.net/jasonzzj/article/details/52017438)

